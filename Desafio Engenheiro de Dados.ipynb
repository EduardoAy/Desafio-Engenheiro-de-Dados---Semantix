{"cells":[{"cell_type":"markdown","source":["# Criação de Função para separação dos logs"],"metadata":{}},{"cell_type":"code","source":["import re\nimport datetime\n\nfrom pyspark.sql import Row\n\nmeses = {'Jan': 1, 'Feb': 2, 'Mar':3, 'Apr':4, 'May':5, 'Jun':6, 'Jul':7,\n    'Aug':8,  'Sep': 9, 'Oct':10, 'Nov': 11, 'Dec': 12}\n\n\ndef dataehora(s):\n     return datetime.datetime(int(s[7:11]),\n                             meses[s[3:6]],\n                             int(s[0:2]),\n                             int(s[12:14]),\n                             int(s[15:17]),\n                             int(s[18:20]))\n\n\ndef LogsApache(logs):\n    valores = re.search(padraoApache, logs)\n    if valores is None:\n        return (logs, 0)\n    campo = valores.group(9)\n    if campo == '-':\n        size = long(0)\n    else:\n        size = long(valores.group(9))\n    return (Row(\n        host          = valores.group(1),\n        client_id     = valores.group(2),\n        user_id       = valores.group(3),\n        date_time     = dataehora(valores.group(4)),\n        method        = valores.group(5),\n        endpoint      = valores.group(6),\n        protocolo     = valores.group(7),\n        response_code = int(valores.group(8)),\n        content_size  = size\n    ), 1)\n"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":["# Expressão para separar os campos dos logs"],"metadata":{}},{"cell_type":"code","source":["padraoApache = '^(\\S+) (\\S+) (\\S+) \\[([\\w:/]+\\s[+\\-]\\d{4})\\] \"(\\S+) (\\S+)\\s*(\\S*)\" (\\d{3}) (\\S+)'"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["# Criação de RDD"],"metadata":{}},{"cell_type":"code","source":["import sys\nimport os\n\ndef criaRdd():\n    log = (sc.textFile(\"/FileStore/tables/nudjwv221501975860023/*.gz\")\n                   .map(LogsApache)\n                   .cache())\n    \n    logs_acesso = (log.filter(lambda s: s[1] == 1).map(lambda s: s[0]).cache())\n\n    logs_falha = (log.filter(lambda s: s[1] == 0).map(lambda s: s[0]))\n    return log, logs_acesso, logs_falha\n\n\nlog, logs_acesso, logs_falha = criaRdd()"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["# Hosts Únicos"],"metadata":{}},{"cell_type":"code","source":["hosts = logs_acesso.map(lambda log: log.host)\n\nhostsUnicos= hosts.distinct()\n\ncontagemHosts = hostsUnicos.count()\nprint 'Hosts Únicos: %d' % contagemHosts"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["# Total de Erros 404"],"metadata":{}},{"cell_type":"code","source":["erros = (logs_acesso.filter(lambda log: log.response_code==404).cache())\nprint 'Total de %d URLs 404' % erros.count()"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["# Top 5 URLs que causaram erros 404"],"metadata":{}},{"cell_type":"code","source":["topUrls = erros.map(lambda log: (log.endpoint,1))\n\nSomaTopUrls = topUrls.reduceByKey(lambda a,b:a+b)\n\ntop5Urls = SomaTopUrls.takeOrdered(5,lambda s: -s[1])\nprint 'Top 5 URLs com erros 404: %s' % top5Urls"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["# Quantidade de Erros 404 por dia"],"metadata":{}},{"cell_type":"code","source":["errosDia = erros.map(lambda log:(log.date_time.day,1))\n\nSomaErros = errosDia.reduceByKey(lambda a,b: a+b)\n\nordemErros = (SomaErros.sortByKey().cache())\n\nerrosDiarios = ordemErros.take(20)\nprint 'Erros 404 por dia: %s' % errosDiarios\n"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["# Total de Bytes Retornados"],"metadata":{}},{"cell_type":"code","source":["totalBytes = logs_acesso.map(lambda log: log.content_size).cache()\nSomaTotal= totalBytes.reduce(lambda a, b : a + b)\nprint 'Total de Bytes Retornados:Max: %s' % SomaTotal"],"metadata":{},"outputs":[],"execution_count":16}],"metadata":{"name":"Desafio Engenheiro de Dados","notebookId":1312697347732923},"nbformat":4,"nbformat_minor":0}
